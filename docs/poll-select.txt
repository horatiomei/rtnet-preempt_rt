Author: L-C. Duca
SPDX-License-Identifier: CC-BY-SA

poll and select
https://programming.vip/docs/linux-kernel-notes-epoll-implementation-principle.html
https://idndx.com/2014/09/01/the-implementation-of-epoll-1
https://jvns.ca/blog/2017/06/03/async-io-on-linux--select--poll--and-epoll

Linux 5.4.5

fs/select.c
struct pollfd used also in user space
struct pollfd {
    int fd;
    short events; /* requested events */
    short revents; /* returned events */
};
struct poll_list {
    struct poll_list *next;
    int len;
    struct pollfd entries[0];
};
SYSCALL_DEFINE3(poll_rtnet, struct pollfd __user *, ufds, unsigned int, nfds,
       int, timeout_msecs) {
if (timeout_msecs >= 0) 
poll_select_set_timeout(...);
    ret = do_sys_poll_rtnet(ufds, nfds, to);
    ...
}
static int do_sys_poll(struct pollfd __user *ufds, unsigned int nfds,
        struct timespec64 *end_time)
{
Builds struct poll_list *walk pe baza ufds & nfds; head is the start of the list.
struct poll_wqueues table;

poll_initwait(&table);
fdcount = do_poll(head, &table, end_time);
poll_freewait(&table);

for (walk = head; walk; walk = walk->next) {
    struct pollfd *fds = walk->entries;
for (j = 0; j < walk->len; j++, ufds++)
Saves fds[j].revents in ufds->revents;
}
}
void poll_initwait_rtnet(struct poll_wqueues_rtnet *pwq)
{
    /* dirty type cast to poll_queue_proc, in reality is not true */
    init_poll_funcptr(&pwq->pt, (poll_queue_proc)__pollwait_rtnet);
    pwq->polling_task = current;
    …
}
poll.h::init_poll_funcptr(poll_table *pt, poll_queue_proc qproc)
{
    pt->_qproc = qproc;
    pt->_key   = ~(__poll_t)0; /* all events enabled */
}
typedef void (*poll_queue_proc)(struct file *, wait_queue_head_t *, struct poll_table_struct *);
static void __pollwait_rtnet(struct file *filp, wait_queue_head_rtnet_t *wait_address,
                poll_table *p)
{
    struct poll_wqueues_rtnet *pwq = container_of(p, struct poll_wqueues_rtnet, pt);
    struct poll_table_entry_rtnet *entry = poll_get_entry_rtnet(pwq);
    if (!entry)
        return;
    entry->filp = get_file(filp);
    entry->wait_address = wait_address;
    entry->key = p->_key;
    init_waitqueue_func_entry(&entry->wait, pollwake_rtnet);
    entry->wait.private = pwq;
    add_wait_queue_rtnet(wait_address, &entry->wait);
}
poll_freewait_rtnet(struct poll_wqueues_rtnet *pwq) {...} -> free_poll_entry_rtnet().
static void free_poll_entry_rtnet(struct poll_table_entry_rtnet *entry)
{
    remove_wait_queue_rtnet(entry->wait_address, &entry->wait);
    fput(entry->filp);
}

static int do_poll(struct poll_list *list, struct poll_wqueues *wait,
           struct timespec64 *end_time)
{
poll_table* pt = &wait->pt;
for(;;) {
    for (walk = list; walk != NULL; walk = walk->next) {
        struct pollfd * pfd, * pfd_end;
        pfd = walk->entries;
        pfd_end = pfd + walk->len;
        for (; pfd != pfd_end; pfd++) {
            if (do_pollfd(pfd, pt, &can_busy_loop, busy_flag)) {
                count++;
                pt->_qproc = NULL;
                …
            }
        }
    }
    pt->_qproc = NULL;
    if (count || timed_out)
        break;
    // set to.
    if (!poll_schedule_timeout(wait, TASK_INTERRUPTIBLE, to, slack))
            timed_out = 1;
}
return count;
}
static int poll_schedule_timeout(struct poll_wqueues *pwq, int state,
              ktime_t *expires, unsigned long slack)
{
int rc=-EINTR;
set_current_state(state);
    if (!pwq->triggered)
        rc = schedule_hrtimeout_range(expires, slack, HRTIMER_MODE_ABS);

    __set_current_state(TASK_RUNNING);
smp_store_mb(pwq->triggered, 0);
    return rc;
}

/*
 * Fish for pollable events on the pollfd->fd file descriptor. We're only
 * interested in events matching the pollfd->events mask, and the result
 * matching that mask is both recorded in pollfd->revents and returned. The
 * pwait poll_table will be used by the fd-provided poll handler for waiting,
 * if pwait->_qproc is non-NULL.
 */
static inline __poll_t do_pollfd(struct pollfd *pollfd, poll_table *pwait,
                     bool *can_busy_poll,
                     __poll_t busy_flag);
{
int fd = pollfd->fd;
    __poll_t mask = 0, filter;
    struct fd f;
    f = fdget(fd);
    mask = vfs_poll(f.file, pwait);
mask &= filter;        /* Mask out unneeded events. */
    fdput(f);
out:
return mask;
}
static inline __poll_t vfs_poll(struct file *file, struct poll_table_struct *pt)
{
return file->f_op->poll(file, pt);
}
socket.c::
static const struct file_operations socket_file_ops_rtnet = {
    .poll =        sock_poll_rtnet,
}
sock_poll_rtnet()->poll.h::static inline void poll_wait_rtnet(struct file * filp, wait_queue_head_rtnet_t * wait_address, poll_table *p) {
if (p && p->_qproc && wait_address)
        p->_qproc(filp, (wait_queue_head_t*) wait_address, p);
}

rt_udp_rcv()->wake_up_interruptible_sync_poll_rtnet(x, m)= __wake_up_sync_key_rtnet((x), TASK_INTERRUPTIBLE, 1, poll_to_key(m))
__wake_up_sync_key_rtnet() -> __wake_up_common_lock_rtnet()
__wake_up_common_lock_rtnet() -> __wake_up_common_rtnet()
__wake_up_common_rtnet(struct wait_queue_head_rtnet *wq_head, unsigned int mode,
            int nr_exclusive, int wake_flags, void *key,
            wait_queue_entry_t *bookmark) identica cu __wake_up_common()
{
    wait_queue_entry_t *curr, *next;
    ...
    if 
        curr = list_next_entry(bookmark, entry);
else
curr = list_first_entry(&wq_head->head, wait_queue_entry_t, entry);
…
list_for_each_entry_safe_from(curr, next, &wq_head->head, entry) {
    ret = curr->func(curr, mode, wake_flags, key); /* pollwake_rtnet() */
    if (ret && (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
               break;
if (bookmark && (++cnt > WAITQUEUE_WALK_BREAK_CNT) &&
                (&next->entry != &wq_head->head)) {
            bookmark->flags = WQ_FLAG_BOOKMARK;
                list_add_tail(&bookmark->entry, &next->entry);
            break;
            }

}

static int __pollwake_rtnet(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
{
    struct poll_wqueues_rtnet *pwq = wait->private;
    DECLARE_WAITQUEUE(dummy_wait, pwq->polling_task);

    /*
     * smp_wmb() is equivalent to smp_wmb() in try_to_wake_up()
     * and is paired with smp_store_mb() in poll_schedule_timeout_rtnet
     */
    smp_wmb();
    pwq->triggered = 1;

    /*
     * Perform the default wake up operation using a dummy waitqueue.
     * TODO: This is hacky but there currently is no interface to
     * pass in @sync.  @sync is scheduled to be removed and once
     * that happens, wake_up_process() can be used directly.
     */
    return default_wake_function(&dummy_wait, mode, sync, key);
}

static int pollwake_rtnet(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
{
    struct poll_table_entry_rtnet *entry;

    entry = container_of(wait, struct poll_table_entry_rtnet, wait);
    if (key && !(key_to_poll(key) & entry->key))
        return 0;
    return __pollwake_rtnet(wait, mode, sync, key);
}

SYSCALL_DEFINE5(select_rtnet, int, n, fd_set __user *, inp, fd_set __user *, outp,
        fd_set __user *, exp, struct timeval __user *, tvp)
{
    return kern_select(n, inp, outp, exp, tvp);
}
fd_set set; /* userspace */

kern_select() 
{
    poll_select_set_timeout(to, …);
    ret = core_sys_select(n, inp, outp, exp, to);
    return poll_select_finish(&end_time, tvp, PT_TIMEVAL, ret);
}

poll_select_finish()  /* computes and verifies time values */

core_sys_select(n, inp, outp, exp, to);
{
fd_set_bits fds;
...
    if ((ret = get_fd_set(n, inp, fds.in)) ||
        (ret = get_fd_set(n, outp, fds.out)) ||
        (ret = get_fd_set(n, exp, fds.ex)))
        goto out;
    zero_fd_set(n, fds.res_in); 
    zero_fd_set(n, fds.res_out);
    zero_fd_set(n, fds.res_ex);
    ...
    ret = do_select(n, &fds, end_time);
}

static int do_select(int n, fd_set_bits *fds, struct timespec64 *end_time)
{
    struct poll_wqueues table;
    poll_table *wait;
    ...
    poll_initwait(&table);
wait = &table.pt;
    for (;;) {
        unsigned long *rinp, *routp, *rexp, *inp, *outp, *exp;
        inp = fds->in; outp = fds->out; exp = fds->ex;
        rinp = fds->res_in; routp = fds->res_out; rexp = fds->res_ex;
        for (i = 0; i < n; ++rinp, ++routp, ++rexp) {
            for (j = 0; j < BITS_PER_LONG; ++j, ++i, bit <<= 1) {
                struct fd f;
                f = fdget(i);
                if (f.file) {
                    wait_key_set(wait, in, out, bit,
                             busy_flag);
                    mask = vfs_poll(f.file, wait);
                    fdput(f);
                    if ((mask & POLLIN_SET) && (in & bit)) {
                        res_in |= bit;
                        retval++;
                        wait->_qproc = NULL;
                    }
                    /* the same for out, ex as for in */
                } /* if */
            } /* for j */
            if (res_in)
                *rinp = res_in;
            /* the same for routp, rexp */
        } /* for i */
    …
        if (!poll_schedule_timeout(&table, TASK_INTERRUPTIBLE,
                       to, slack))
            timed_out = 1;
    }

    poll_freewait(&table);
    return retval;
}

struct poll_table_entry_rtnet *poll_get_entry_rtnet(struct poll_wqueues_rtnet *p) {
    struct poll_table_page_rtnet *table = p->table;

    if (p->inline_index < N_INLINE_POLL_ENTRIES)
        return p->inline_entries + p->inline_index++;

    if (!table || POLL_TABLE_FULL(table)) {
        struct poll_table_page_rtnet *new_table;
     ...
        new_table = (struct poll_table_page_rtnet *) __get_free_page(GFP_KERNEL);
 ...
        p->table = new_table;  table = new_table;
    }
    return table->entry++;
}

kernel/sched/wait.c
void __init_waitqueue_head_rtnet(struct wait_queue_head_rtnet *wq_head,
                                 const char *name, struct lock_class_key *key)
{
    raw_spin_lock_init(&wq_head->lock);
    INIT_LIST_HEAD(&wq_head->head);
}

fs/select.c
struct poll_table_page_rtnet {
    struct poll_table_page_rtnet * next;
    struct poll_table_entry_rtnet * entry;
    struct poll_table_entry_rtnet entries[0];
};

include/linux/poll.h
struct poll_table_entry_rtnet {
    struct file *filp;
    __poll_t key;
    wait_queue_entry_t wait;
    wait_queue_head_rtnet_t *wait_address;
};

struct poll_wqueues_rtnet {
    poll_table pt;
    struct poll_table_page_rtnet *table;
    struct task_struct *polling_task;
    int triggered;
    int error;
    int inline_index;
    struct poll_table_entry_rtnet inline_entries[N_INLINE_POLL_ENTRIES];
};

typedef struct poll_table_struct {
    poll_queue_proc _qproc;
    __poll_t _key;
} poll_table;

include/linux/wait.h
struct wait_queue_head_rtnet {
    raw_spinlock_t lock;
    struct list_head    head;
};
struct wait_queue_entry {
    unsigned int        flags;
    void            *private;
    wait_queue_func_t    func;
    struct list_head    entry;
};

static inline int waitqueue_active_rtnet(struct wait_queue_head_rtnet *wq_head)
{
    return !list_empty(&wq_head->head);
}

static inline bool wq_has_sleeper_rtnet(struct wait_queue_head_rtnet *wq_head)
{
    smp_mb();
    return waitqueue_active_rtnet(wq_head);
}

static inline void __add_wait_queue_rtnet(struct wait_queue_head_rtnet *wq_head, struct wait_queue_entry *wq_entry)
{
    list_add(&wq_entry->entry, &wq_head->head);
}



https://stackoverflow.com/questions/30035776/how-to-add-poll-function-to-the-kernel-module-code
#include <linux/wait.h>
#include <linux/poll.h>
Declare waitqueue variable:
static DECLARE_WAIT_QUEUE_HEAD(fortune_wait);
Add fortune_poll() function and add it (as .poll callback) to your file operations structure:
static unsigned int fortune_poll(struct file *file, poll_table *wait)
{
    poll_wait(file, &fortune_wait, wait);
    if (new-data-is-ready)
        return POLLIN | POLLRDNORM;
    return 0;
}

static const struct file_operations proc_test_fops = {
    ....
    .poll = fortune_poll,
};
Note that you should return POLLIN | POLLRDNORM if you have some new data to read, and 0 in case there is no new data to read (poll() call timed-out).
Notify your waitqueue once you have new data:
wake_up_interruptible(&fortune_wait);

